{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Types of Activation Functions in Neural Network\n",
    "\n",
    "Activation functions are the most crucial part of any neural network in deep learning. In deep learning, very complicated tasks are image classification, language transformation, object detection, etc which are needed to address with the help of neural networks and activation function.\n",
    "\n",
    "## Types of Activation Functions\n",
    " \n",
    "\n",
    "We have divided all the essential neural networks in three major parts:\n",
    "\n",
    "A. Binary step function\n",
    "\n",
    "B. Linear function\n",
    "\n",
    "C. Non linear activation function\n",
    "\n",
    "### A. Binary Step Neural Network Activation Function\n",
    " \n",
    "#### 1. Binary Step Function\n",
    " \n",
    "This activation function very basic and it comes to mind every time if we try to bound output. It is basically a threshold base classifier, in this, we decide some threshold value to decide output that neuron should be activated or deactivated.\n",
    "\n",
    "![binaryfunc](https://lh5.googleusercontent.com/dteMmG-w6jMsTiJLRRxnbH7PIQIUq5EnTY5_LgOD4IjrsZyznuCNWklLWKZ3c2BGVfRT0jkPeaIYcpYNjgwNJEUrMSslrI6BbVTuR1oiBXauSrN3PyVlZLNoOO8ozJhj7Dlsh3iN) \n",
    "\n",
    "$f(x) = 1$ if $x >$ 0  \n",
    "else 0 if $x < $ 0 $\n",
    "\n",
    "In this, we decide the threshold value to 0. It is very simple and useful to classify binary problems or classifier.\n",
    "\n",
    "\n",
    "### B. Linear Neural Network Activation Function\n",
    " \n",
    "#### 2. Linear Function\n",
    " \n",
    "It is a simple straight line activation function where our function is directly proportional to the weighted sum of neurons or input. Linear activation functions are better in giving a wide range of activations and a line of a positive slope may increase the firing rate as the input rate increases.\n",
    "\n",
    "In binary, either a neuron is firing or not. If you know gradient descent in deep learning then you would notice that in this function derivative is constant.\n",
    "\n",
    "$Y = mZ$\n",
    "\n",
    "Where derivative with respect to Z is constant m. The meaning gradient is also constant and it has nothing to do with Z. In this, if the changes made in backpropagation will be constant and not dependent on Z so this will not be good for learning. \n",
    "\n",
    "In this, our second layer is the output of a linear function of previous layers input. Wait a minute, what have we learned in this that if we compare our all the layers and remove all the layers except the first and last then also we can only get an output which is a linear function of the first layer.\n",
    "\n",
    "### C. Non Linear Neural Network Activation Function\n",
    " \n",
    "#### 3. ReLU( Rectified Linear unit) Activation function\n",
    " \n",
    "Rectified linear unit or ReLU is most widely used activation function right now which ranges from 0 to infinity, All the negative values are converted into zero, and this conversion rate is so fast that neither it can map nor fit into data properly which creates a problem, but where there is a problem there is a solution.\n",
    "\n",
    "![relu](https://lh6.googleusercontent.com/EZcM_eYogXcBnc2Ksq-v96Vkc9hDDZcOIMAKFBviFSRZf0lHMH-QZ320bgAeXcrzyPsjldDpO7tbkRKalWoRWk2rZboV45B2_lcA4-g5NP598_X9hBzzodNnM6q6lz4FyGPxhydh)\n",
    " \n",
    "We use Leaky ReLU function instead of ReLU to avoid this unfitting, in Leaky ReLU range is expanded which enhances the performance.\n",
    "\n",
    "\n",
    "#### 4. Leaky ReLU Activation Function\n",
    " \n",
    "![LRelu](https://lh4.googleusercontent.com/80MrETh1PCKnwG9LzvuwDjKB3RP9E3y8Ghai_lFoaaJiWhrHZ5byujOzYYFVev3vIxoy-ObqjAavFM-aBIcbXVWToMhWu8r8eEEOl8bJdZ-joTIjAlQnbvpzZFmBD7RMtS5JiDRL)\n",
    "\n",
    "We needed the Leaky ReLU activation function to solve the ‘Dying ReLU’ problem, as discussed in ReLU, we observe that all the negative input values turn into zero very quickly and in the case of Leaky ReLU we do not make all negative inputs to zero but to a value near to zero which solves the major issue of ReLU activation function.\n",
    "\n",
    "#### 5. Sigmoid Activation Function\n",
    " \n",
    "The sigmoid activation function is used mostly as it does its task with great efficiency, it basically is a probabilistic approach towards decision making and ranges in between 0 to 1, so when we have to make a decision or to predict an output we use this activation function because of the range is the minimum, therefore, prediction would be more accurate.\n",
    "![sigmoid](https://lh4.googleusercontent.com/6OrAFufn0HyLY1Rfn36OuZNG-4BRYuFqJyGNTRl6CW0fwFizWK36Nk4uxFfCpxGi5H-8XigIPbXaS9JPCu2TYjbCDnYzZ_qRiIjthfi42sDNcErW9AgNaTroit6CjEknHJxO914J)\n",
    "\n",
    "The equation for the sigmoid function is\n",
    "\n",
    " \n",
    "\n",
    "$f(x) = 1/(1+e(-x) )$\n",
    "\n",
    " \n",
    "\n",
    "The sigmoid function causes a problem mainly termed as vanishing gradient problem which occurs because we convert large input in between the range of 0 to 1 and therefore their derivatives become much smaller which does not give satisfactory output. To solve this problem another activation function such as ReLU is used where we do not have a small derivative problem.\n",
    "\n",
    "#### 6. Hyperbolic Tangent Activation Function(Tanh)\n",
    "   \n",
    "   ![Tanh](https://lh6.googleusercontent.com/DcCGRp1XSzCaI8k614tJv_96dSUCiPBbncukrqzsvqhCQlxwubc2iB2xIcBLeXFElHTT1w5ejPkrlV-ye2RKFkJL5l6mzw28fp-T1VBX5Z9Up-3KJcRV9dIz8K3xO_WZ2-F9L3xX)\n",
    "\n",
    "This activation function is slightly better than the sigmoid function, like the sigmoid function it is also used to predict or to differentiate between two classes but it maps the negative input into negative quantity only and ranges in between -1 to  1.\n",
    "\n",
    "\n",
    "#### 7. Softmax Activation Function\n",
    " \n",
    "Softmax is used mainly at the last layer i.e output layer for decision making the same as sigmoid activation works, the softmax basically gives value to the input variable according to their weight and the sum of these weights is eventually one.\n",
    "\n",
    "![softmax](https://lh5.googleusercontent.com/IL52WzEWdVcfhpKScPD-pUE3VTDbPjo3Genu5I1REyrdhEQ0HGQulOvdMF2NnEXndQov-h7qKWwheg-2y0O-4Od0AZ16BTp2mZIAHwRRKgGT7NxZzhc2HSgkHYHtxZXUX2RAFpao)\n",
    "\n",
    "For Binary classification, both sigmoid, as well as softmax, are equally approachable but in case of multi-class classification problem we generally use softmax and cross-entropy along with it.\n",
    "\n",
    "C:\\Users\\Komal Khan\\AppData\\Local\\Programs\\Python\\Python39\n",
    "C:\\Users\\Komal Khan\\AppData\\Local\\Programs\\Python\\Python39\\Scripts"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56290c1378915fbf1ccc41d57d6da8ce7d56d5d08b6d0ffe5f7d99f223a5c1f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
